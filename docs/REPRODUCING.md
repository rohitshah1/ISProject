# Reproducing This Analysis

## System Requirements and Setup

This analysis can be reproduced on systems running macOS, Linux, or Windows with Windows Subsystem for Linux. The workflow requires Python version 3.8 or higher, with our development and testing conducted using Python 3.11.1. Git version control software is needed for cloning the repository, and systems should have a minimum of 500 megabytes of free disk space to accommodate raw data, processed outputs, and results. The complete set of Python package dependencies is specified in the requirements.txt file and includes pandas version 1.5.0 or higher for data manipulation, numpy 1.23.0 or higher for numerical operations, requests 2.28.0 or higher for API interactions, matplotlib 3.6.0 or higher and seaborn 0.12.0 or higher for visualization, scikit-learn 1.2.0 or higher for statistical utilities, statsmodels 0.14.0 or higher for regression analysis, snakemake 7.18.0 or higher for workflow management, and pyarrow 10.0.0 or higher for efficient data serialization.

The first step in reproduction is obtaining the project code by cloning the repository from GitHub using the command "git clone https://github.com/drudata/ISProject.git" followed by changing into the project directory with "cd ISProject". Alternatively, users can download and extract the ZIP archive directly from GitHub if they prefer not to use Git. We strongly recommend creating a Python virtual environment to isolate dependencies and avoid conflicts with other projects. This can be accomplished by running "python3 -m venv venv" to create the environment, then activating it using "source venv/bin/activate" on macOS and Linux or "venv\\Scripts\\activate" on Windows. Once the virtual environment is active, all required packages can be installed by running "pip install -r requirements.txt", which reads the dependencies file and installs the appropriate versions. Users can verify successful installation by running "python3 -c 'import pandas, numpy, requests; print(\"Dependencies installed successfully\")'" which will print a confirmation message if all core packages are available.

## Obtaining API Credentials

Two API keys are required to download data from the original sources. The NOAA Climate Data Online API token can be obtained by visiting https://www.ncdc.noaa.gov/cdo-web/token, entering an email address, and checking email for the token which typically arrives within minutes. The token is a 32-character alphanumeric string that must be kept secure. The USDA NASS QuickStats API key is obtained by visiting https://quickstats.nass.usda.gov/api and requesting a key using an email address, with the key arriving immediately via email as a 36-character string containing hyphens. Both of these keys provide free access to public data but have usage limits to prevent abuse, so they should not be shared publicly or committed to version control systems.

Configuring the API keys can be done interactively or manually depending on user preference. For interactive setup, running "python3 scripts/setup_api_keys.py" launches a guided process that prompts for each key and automatically updates the config.py file with the provided values. For manual configuration, users can directly edit the config.py file and replace the placeholder text "your_noaa_token_here" with their actual NOAA token and "your_usda_key_here" with their actual USDA key. After configuration by either method, running "python3 config.py" will verify that both keys are properly configured by printing checkmarks next to each successfully detected key.

The required directory structure should already exist in the cloned repository, but users should verify that directories exist for data/raw, data/processed, data/metadata, results, scripts, workflow, and docs. If any directories are missing, they can be created using "mkdir -p data/{raw,processed,metadata} results scripts workflow docs" which creates the entire hierarchy in a single command.

## Data Acquisition Options

Users have two options for obtaining the datasets depending on whether they want to use preprocessed data for quick reproduction or download fresh data directly from APIs for complete reproduction. If using provided data files that may be distributed with the project or available via Box, users should verify that data/raw/noaa_full.csv and data/raw/usda_yields.csv exist and contain data before proceeding. When these files are present, the data acquisition step can be skipped, and users can proceed directly to running the analysis pipeline.

For complete reproduction involving fresh data downloads, users must run the acquisition scripts that interact with the NOAA and USDA APIs. Running "python3 scripts/get_noaa_data.py" downloads daily weather observations from NOAA, a process that typically requires 10 to 20 minutes depending on network speed and API response times. The script displays progress through logging output showing how many years and days have been retrieved. The resulting file data/raw/noaa_full.csv should be approximately 90 megabytes containing roughly 1.9 million rows of daily weather observations. Subsequently running "python3 scripts/get_usda_data.py" downloads county-level crop yield data from USDA, which typically completes in 1 to 2 minutes and produces a file data/raw/usda_yields.csv of approximately 400 kilobytes containing 1,210 rows. The specific time range for downloads is configurable in config.py through the DATA_START_YEAR and DATA_END_YEAR variables, which default to 2018 and 2023 respectively. Users wanting complete historical coverage can adjust these to 1990 and 2023, though downloads will take proportionally longer for extended time periods.

## Running the Analysis Pipeline

Once data is acquired through either method, the complete analysis can be executed using one of three approaches depending on user preference and familiarity with workflow tools. The recommended approach uses the automation script by simply running "./workflow/run_all.sh" which executes all processing steps in sequence including data cleaning for both NOAA and USDA datasets, dataset integration on common keys, data quality profiling, statistical analysis with multiple regression models, visualization generation, and checksum computation for data integrity. This comprehensive pipeline typically completes in 2 to 5 minutes on standard hardware. If users encounter a permission error indicating the script is not executable, running "chmod +x workflow/run_all.sh" first will make it executable.

Alternatively, the Snakemake workflow management system can orchestrate execution by running "snakemake --cores 1" which analyzes dependencies between steps and only re-runs portions of the pipeline if input data has changed. Snakemake provides intelligent caching that can save time during iterative development. Users with multi-core processors can parallelize independent steps by specifying "--cores 4" or another appropriate number.

For maximum control and transparency, users may prefer manual step-by-step execution where each script is run individually in sequence. This involves first running "python3 scripts/clean_noaa.py" to process weather data, then "python3 scripts/clean_usda.py" to process yield data, followed by "python3 scripts/integrate_datasets.py" to merge the cleaned datasets, "python3 scripts/profile_data.py" to assess data quality, "python3 scripts/analyze_data.py" to run statistical models, "python3 scripts/visualize_results.py" to generate plots, and finally "python3 scripts/verify_data.py compute" to calculate checksums. This manual approach provides the clearest view of each processing stage and makes it easy to inspect intermediate outputs.

## Verifying Results

After pipeline execution completes successfully, users should verify that all expected outputs were created correctly. The processed data files should include data/processed/noaa_clean.csv containing 34 rows of annual state-level climate metrics, data/processed/usda_clean.csv with 1,210 rows of cleaned county-year-crop yields, and data/processed/integrated.csv with 1,085 rows representing the analysis-ready merged dataset. Analysis outputs should include results/analysis_results.json containing regression coefficients, R-squared values, and p-values, along with results/data_profile.json documenting quality scores, missing data patterns, and outlier detection results. The results directory should contain six visualization files including crop_yield_vs_volatility.png showing scatter plots with regression lines, temporal_trends.png displaying time series of yields and volatility, correlation_matrix.png as a heatmap of variable associations, volatility_distribution.png with histograms by crop type, crop_yield_by_quartile.png showing box plots across volatility levels, and geographic_summary.png presenting county-level summaries. Finally, the data/checksums.json file contains SHA-256 cryptographic hashes for verifying data integrity.

To formally verify data integrity and ensure that files match the original analysis, users can run "python3 scripts/verify_data.py verify" which computes checksums for all data files and compares them against stored values, printing verification status for each file and confirming if all checksums match. It is important to note that checksums will naturally differ if data was downloaded at a different time since USDA updates their database annually with new crop years, or if different date ranges were specified in the configuration file. Nevertheless, the verification process remains useful for ensuring data has not been corrupted or modified unexpectedly.

Reviewing results can be done through various methods depending on user preferences. The statistical model results can be examined in formatted JSON by running "cat results/analysis_results.json | python3 -m json.tool" which displays the hierarchical structure with proper indentation. Similarly, data quality assessment can be viewed using "cat results/data_profile.json | python3 -m json.tool". Visualizations can be opened using any standard image viewer application on the operating system. Comprehensive documentation of findings and interpretation is available in docs/DATA_QUALITY.md describing quality assessment details, docs/FINDINGS.md explaining research findings and their interpretation, and docs/FUTURE_WORK.md discussing lessons learned and future research directions.

## Troubleshooting Common Issues

Several common issues may arise during reproduction, and we provide solutions for the most frequently encountered problems. If API rate limiting occurs causing downloads to fail or slow dramatically, this reflects the fact that NOAA limits requests to 5 per second and USDA allows 1,000 requests per day. Our scripts include appropriate delays to respect these limits, but users encountering problems can reduce the date range in config.py, schedule data downloads during off-peak hours, or contact API providers to request increased rate limits for research purposes. If users encounter module not found errors indicating missing dependencies, they should try re-installing requirements using "pip install -r requirements.txt", and if issues persist they should first upgrade pip itself using "pip install --upgrade pip" before attempting to reinstall requirements.

Permission denied errors when attempting to run the automation script can be resolved by making it executable with "chmod +x workflow/run_all.sh" before running it again. Some systems have "python" pointing to Python version 2 rather than Python 3, so users encountering "command not found: python" errors should consistently use "python3" instead throughout all commands. If memory errors occur when processing the large NOAA file, the clean_noaa.py script uses chunked reading to handle large datasets, but users with limited RAM can further reduce the chunksize parameter in the script, close other applications to free memory, or use a machine with more RAM, as we recommend at least 4 gigabytes for smooth processing. Visualization files are saved to disk rather than displayed interactively by design, so users expecting interactive plots should instead look for saved PNG files in the results directory.

If numerical results differ from those documented, several explanations are possible. Users who downloaded fresh data from APIs may have different values because USDA periodically updates recent years' data as more complete information becomes available. Different date ranges specified in config.py will naturally produce different results. Minor numerical differences are also expected due to different package versions, as floating-point arithmetic can vary slightly across software versions even when algorithms are identical. Users can verify their configuration by checking the date range with "grep 'DATA_.*_YEAR' config.py" and checking package versions using "pip list | grep -E '(pandas|numpy|statsmodels)'".

## Expected Outputs and Success Criteria

A successful reproduction should produce data files with specific characteristics. The file data/raw/noaa_full.csv should be approximately 90 megabytes containing 1.9 million rows of daily weather observations. The file data/raw/usda_yields.csv should be roughly 400 kilobytes with 1,210 rows of county-level yields. After processing, data/processed/noaa_clean.csv should contain 34 rows of annual climate summaries at approximately 5 kilobytes, data/processed/usda_clean.csv should have 1,210 rows of cleaned yields at 34 kilobytes, and data/processed/integrated.csv should contain 1,085 rows of merged data at 118 kilobytes. Analysis outputs include results/analysis_results.json with regression statistics and results/data_profile.json with quality assessments, along with data/checksums.json containing SHA-256 hashes for all data files. Six visualization PNG files should be created showing various aspects of the climate-yield relationships.

Reproduction is considered successful if all scripts execute without errors, all expected output files are created with reasonable sizes, visualizations display clear patterns including negative relationships between yield and volatility, and statistical models show significant effects for crop-specific analyses. Specifically, regression results should show approximately the following patterns, though minor numerical differences are acceptable. For corn, the temperature standard deviation coefficient should be approximately negative 6.5, the p-value should be less than 0.001, and the R-squared should be near 0.18. For soybeans, the temperature standard deviation coefficient should be approximately negative 2.0, the p-value should be less than 0.001, and the R-squared should be near 0.11. Minor numerical differences from these values are expected due to software version variations and floating-point arithmetic precision, but overall patterns and statistical significance should match closely.

## Computational Environment Documentation

For complete transparency regarding the environment used for our original analysis, we document that the operating system was macOS 24.0.0 running the Darwin kernel, Python version was 3.11.1, and the shell environment was Z shell (zsh). Key package versions included pandas 1.5.3, numpy 1.24.2, statsmodels 0.14.0, matplotlib 3.7.1, and seaborn 0.12.2. Standard laptop hardware with 4 to 8 gigabytes of RAM is sufficient for all processing steps. Users wishing to match our environment exactly can force reinstall the specific versions listed in requirements.txt, though we expect results to be robust across reasonable version variations for these stable, mature packages.

## Partial Reproduction Options

Users interested in reproducing only specific portions of the analysis can execute individual steps independently assuming they have the necessary input data. To regenerate only the visualizations using provided integrated data, simply run "python3 scripts/visualize_results.py" which reads the integrated dataset and creates all six plots. To rerun only the statistical analysis without reprocessing raw data, execute "python3 scripts/analyze_data.py" which performs all regression analyses and generates the results JSON file. To reintegrate previously cleaned datasets, run "python3 scripts/integrate_datasets.py" which merges NOAA and USDA cleaned files. This modularity allows users to focus on specific aspects of the workflow that interest them without necessarily executing the complete pipeline from raw data acquisition through final visualization.

## Citation and Acknowledgment

Users who utilize this workflow, code, or data in their own research should provide appropriate citation. The recommended citation format is "Udata, D.R. & Shah, R. (2025). Climate Variability and Agricultural Productivity in Illinois: An Integrated Data Analysis. IS477 Course Project, University of Illinois at Urbana-Champaign. GitHub: https://github.com/drudata/ISProject". This project was conducted as part of the IS477 Data Management, Curation, and Reproducibility course at the School of Information Sciences, and we acknowledge the course instructors and teaching assistants for their guidance in reproducible research practices.

## Conclusion

This comprehensive reproduction guide provides multiple pathways for users with different levels of technical expertise and different goals to successfully reproduce our analysis. The combination of automated scripts, workflow management tools, and detailed documentation should enable both exact reproduction of our published results and adaptation of the workflow for related research questions or geographic regions. The modular design ensures that users can work with either complete raw data downloads or preprocessed files, and can execute either the full pipeline or individual components depending on their specific needs and computational constraints.
